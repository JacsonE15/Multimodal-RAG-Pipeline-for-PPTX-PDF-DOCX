{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multimodal RAG Pipeline\n",
    "\n",
    "## 1. Environment Setup:\n",
    "\n",
    "This notebook implements a **minimal end-to-end Retrieval-Augmented Generation (RAG)** system\n",
    "for PDF / PPT / Word documents, enhanced with Vision-Language Models (VLM).\n",
    "\n",
    "At this stage, we focus on:\n",
    "- Document parsing\n",
    "- Text & image understanding\n",
    "- Text embedding and vector retrieval\n",
    "- Basic RAG question answering\n"
   ],
   "metadata": {
    "id": "hGJ3f7Eqejta"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Install dependencies\n",
    "!pip install -q \\\n",
    "  pypdf \\\n",
    "  python-pptx \\\n",
    "  python-docx \\\n",
    "  sentence-transformers \\\n",
    "  faiss-cpu \\\n",
    "  transformers \\\n",
    "  pillow \\\n",
    "  torch torchvision torchaudio\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x26tf072lrmf",
    "outputId": "d7735d3a-83af-4e09-d329-96ac30214952"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/472.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Check GPU\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ],
   "metadata": {
    "id": "iI-q5grFe9L5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f275c7d9-8838-49fd-a18c-4c10a09ae141"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Upload files\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ],
   "metadata": {
    "id": "7jGHcYpbfOrb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "outputId": "b18d5371-e78e-4481-ac7b-065f9803d06f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-76f93fea-8d85-4d12-bac8-1ce72c4ff4dc\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-76f93fea-8d85-4d12-bac8-1ce72c4ff4dc\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving Lecture 5_IR_Websearch.pdf to Lecture 5_IR_Websearch.pdf\n",
      "Saving Lecture 6 Sentimental Analysis_Short.pdf to Lecture 6 Sentimental Analysis_Short.pdf\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Chunk Extraction (PDF, PPTX, DOCX)\n",
    "\n",
    "Implement three separate pipelines for handling PDF, PPTX, and DOCX files.  \n",
    "Each pipeline consists of two stages:\n",
    "1. Pure text extraction  \n",
    "2. Image extraction\n"
   ],
   "metadata": {
    "id": "0H1PCCAuhT_f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####2.1 PDF pipeline\n",
    "This pipeline extracts text and images from PDF files, organizes them into structured chunks with metadata, and prepares them for embedding and retrieval."
   ],
   "metadata": {
    "id": "6JwYiO-dhJO2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pymupdf"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOKzsMwAypC3",
    "outputId": "30aae057-5464-4419-f3a2-34ee34e1773e"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pypdf import PdfReader\n",
    "import fitz  # pymupdf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def _render_pdf_page_to_pil(doc: fitz.Document, page_index0: int, zoom: float = 2.0) -> Image.Image:\n",
    "    \"\"\"Render one PDF page (0-based index) to PIL.Image\"\"\"\n",
    "    page = doc.load_page(page_index0)\n",
    "    mat = fitz.Matrix(zoom, zoom)\n",
    "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _estimate_image_area_ratio(page: fitz.Page) -> float:\n",
    "    \"\"\"\n",
    "    Estimate image area ratio on a page:\n",
    "    sum(area of image blocks) / page area\n",
    "    Uses PyMuPDF text/image blocks (fast & good enough for routing).\n",
    "    \"\"\"\n",
    "    page_area = float(page.rect.width * page.rect.height) or 1.0\n",
    "\n",
    "    img_area = 0.0\n",
    "    try:\n",
    "        d = page.get_text(\"dict\")\n",
    "        for b in d.get(\"blocks\", []):\n",
    "            if b.get(\"type\") == 1:  # 1 = image block\n",
    "                x0, y0, x1, y1 = b.get(\"bbox\", (0, 0, 0, 0))\n",
    "                img_area += max(0.0, float(x1 - x0)) * max(0.0, float(y1 - y0))\n",
    "    except Exception:\n",
    "        # fallback: if something goes wrong, return 0\n",
    "        return 0.0\n",
    "\n",
    "    ratio = img_area / page_area\n",
    "    # clamp to [0,1] for safety\n",
    "    return max(0.0, min(1.0, ratio))\n",
    "\n",
    "\n",
    "def extract_pdf_chunks(\n",
    "    path: str,\n",
    "    zoom: float = 2.0,\n",
    "    vlm_ratio_threshold: float = 0.10,\n",
    "    keep_textless_image_pages: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Extract PDF chunks with:\n",
    "    - raw_text from PyPDF\n",
    "    - image_area_ratio from PyMuPDF\n",
    "    - page_image (PIL) only when image_area_ratio >= vlm_ratio_threshold (so VLM can work)\n",
    "\n",
    "    Output chunk schema (keeps your fields + adds page_image):\n",
    "    {\n",
    "      \"source_file\": path,\n",
    "      \"type\": \"pdf\",\n",
    "      \"page\": 1-based,\n",
    "      \"raw_text\": \"...\",\n",
    "      \"image_area_ratio\": float,\n",
    "      \"route\": None,\n",
    "      \"page_image\": PIL.Image or None\n",
    "    }\n",
    "    \"\"\"\n",
    "    reader = PdfReader(path)\n",
    "    doc = fitz.open(path)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    n_pages = min(len(reader.pages), doc.page_count)\n",
    "\n",
    "    for i in range(n_pages):\n",
    "        page_num_1based = i + 1\n",
    "\n",
    "        # 1.text (PyPDF)\n",
    "        try:\n",
    "            text = (reader.pages[i].extract_text() or \"\").strip()\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "\n",
    "        # 2.image ratio (PyMuPDF)\n",
    "        try:\n",
    "            page = doc.load_page(i)\n",
    "            image_area_ratio = _estimate_image_area_ratio(page)\n",
    "        except Exception:\n",
    "            image_area_ratio = 0.0\n",
    "\n",
    "        # 3.render page image ONLY if ratio >= threshold\n",
    "        page_image = None\n",
    "        if image_area_ratio >= vlm_ratio_threshold:\n",
    "            try:\n",
    "                page_image = _render_pdf_page_to_pil(doc, i, zoom=zoom)\n",
    "            except Exception:\n",
    "                page_image = None\n",
    "\n",
    "        # 4.write chunk\n",
    "        # - keep textless pages if they have meaningful images and you want them retrievable by VLM caption\n",
    "        if text or (keep_textless_image_pages and page_image is not None):\n",
    "            chunks.append({\n",
    "                \"source_file\": path,\n",
    "                \"type\": \"pdf\",\n",
    "                \"page\": page_num_1based,\n",
    "                \"raw_text\": text,\n",
    "                \"image_area_ratio\": float(image_area_ratio),\n",
    "                \"route\": None,\n",
    "                \"page_image\": page_image,   # <- NEW: for VLM\n",
    "            })\n",
    "\n",
    "    doc.close()\n",
    "    return chunks"
   ],
   "metadata": {
    "id": "_os1gG9yuD66"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "####2.2 PPTX pipeline"
   ],
   "metadata": {
    "id": "uiNN1XpehHG_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import io\n",
    "from typing import List, Dict, Any, Optional\n",
    "from PIL import Image\n",
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
    "\n",
    "_pptx_cache = {}\n",
    "\n",
    "def _get_pptx(path: str) -> Presentation:\n",
    "    if path not in _pptx_cache:\n",
    "        _pptx_cache[path] = Presentation(path)\n",
    "    return _pptx_cache[path]\n",
    "\n",
    "\n",
    "def extract_pptx_chunks(\n",
    "    path: str,\n",
    "    vlm_ratio_threshold: float = 0.10,   # if bigger than 0.1 then go through VLM\n",
    "    max_images_per_slide: int = 3,       # max3 images per page\n",
    "    keep_textless_image_slides: bool = True,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    the form of chunk keep same with pdf：\n",
    "    {\n",
    "      \"source_file\": path,\n",
    "      \"type\": \"pptx\",\n",
    "      \"slide\": 1-based,\n",
    "      \"raw_text\": \"...\",\n",
    "      \"image_area_ratio\": float,\n",
    "      \"route\": None,\n",
    "      \"images\": [PIL.Image, ...]   # <-- 给VLM用（PPTX版的“page_image”集合）\n",
    "    }\n",
    "    \"\"\"\n",
    "    prs = _get_pptx(path)\n",
    "    chunks: List[Dict[str, Any]] = []\n",
    "\n",
    "    # slide area\n",
    "    slide_w = float(prs.slide_width or 1.0)\n",
    "    slide_h = float(prs.slide_height or 1.0)\n",
    "    slide_area = slide_w * slide_h if slide_w > 0 and slide_h > 0 else 1.0\n",
    "\n",
    "    for si, slide in enumerate(prs.slides, start=1):\n",
    "        parts: List[str] = []\n",
    "        images_with_ratio: List[tuple] = []   # [(ratio, PIL.Image), ...]\n",
    "        max_img_ratio = 0.0\n",
    "\n",
    "        for shape in slide.shapes:\n",
    "            # 1.text\n",
    "            if hasattr(shape, \"text\") and shape.text:\n",
    "                t = shape.text.strip()\n",
    "                if t:\n",
    "                    parts.append(t)\n",
    "\n",
    "            # 2.pictures: ratio + extract PIL\n",
    "            if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:\n",
    "                # 2.1ratio\n",
    "                try:\n",
    "                    img_area = float(shape.width * shape.height)\n",
    "                    ratio = img_area / slide_area\n",
    "                    if ratio > max_img_ratio:\n",
    "                        max_img_ratio = ratio\n",
    "                except Exception:\n",
    "                    ratio = 0.0\n",
    "\n",
    "                # 2.1only extract images if ratio is meaningful (>= threshold)\n",
    "                if ratio >= vlm_ratio_threshold:\n",
    "                    try:\n",
    "                        blob = shape.image.blob\n",
    "                        pil_img = Image.open(io.BytesIO(blob)).convert(\"RGB\")\n",
    "                        images_with_ratio.append((ratio, pil_img))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        text = \"\\n\".join(parts).strip()\n",
    "\n",
    "        # only keep max_images_per_slide\n",
    "        if images_with_ratio:\n",
    "            images_with_ratio.sort(key=lambda x: x[0], reverse=True)\n",
    "            images = [im for _, im in images_with_ratio[:max_images_per_slide]]\n",
    "        else:\n",
    "            images = []\n",
    "\n",
    "        # 3.keep policy\n",
    "        keep = bool(text)\n",
    "        if (not keep) and keep_textless_image_slides and (max_img_ratio >= vlm_ratio_threshold):\n",
    "            keep = True\n",
    "\n",
    "        if keep:\n",
    "            chunks.append({\n",
    "                \"source_file\": path,\n",
    "                \"type\": \"pptx\",\n",
    "                \"slide\": si,\n",
    "                \"raw_text\": text,\n",
    "                \"image_area_ratio\": float(max_img_ratio),\n",
    "                \"route\": None,\n",
    "                \"images\": images,   #send image into chunk\n",
    "            })\n",
    "\n",
    "    return chunks"
   ],
   "metadata": {
    "id": "qSs3pgl8vnzo"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "####2.3 DOCX pipeline"
   ],
   "metadata": {
    "id": "LsbJqJ-swM5w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import io\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "from docx import Document\n",
    "from docx.text.paragraph import Paragraph\n",
    "from docx.table import Table\n",
    "\n",
    "\n",
    "def iter_block_items(parent):\n",
    "    parent_elm = parent.element.body\n",
    "    for child in parent_elm.iterchildren():\n",
    "        if child.tag.endswith('}p'):\n",
    "            yield Paragraph(child, parent)\n",
    "        elif child.tag.endswith('}tbl'):\n",
    "            yield Table(child, parent)\n",
    "\n",
    "\n",
    "def extract_images_from_paragraph(paragraph: Paragraph) -> List[Image.Image]:\n",
    "    images = []\n",
    "\n",
    "    # run._element.xml contain drawing / blip\n",
    "    # find a:blip  embed rId，retrive picture from related_parts\n",
    "    try:\n",
    "        for run in paragraph.runs:\n",
    "            # quick filter\n",
    "            if \"a:blip\" not in run._element.xml:\n",
    "                continue\n",
    "\n",
    "            blips = run._element.xpath(\".//a:blip\")\n",
    "            for blip in blips:\n",
    "                rId = blip.attrib.get(\n",
    "                    \"{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed\"\n",
    "                )\n",
    "                if not rId:\n",
    "                    continue\n",
    "\n",
    "                image_part = paragraph.part.related_parts.get(rId)\n",
    "                if image_part is None:\n",
    "                    continue\n",
    "\n",
    "                blob = image_part.blob\n",
    "                try:\n",
    "                    pil_img = Image.open(io.BytesIO(blob)).convert(\"RGB\")\n",
    "                    images.append(pil_img)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def extract_images_from_table(table: Table) -> List[Image.Image]:\n",
    "    images = []\n",
    "    try:\n",
    "        for row in table.rows:\n",
    "            for cell in row.cells:\n",
    "                for p in cell.paragraphs:\n",
    "                    images.extend(extract_images_from_paragraph(p))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return images\n",
    "\n",
    "\n",
    "def extract_docx_chunks(\n",
    "    path: str,\n",
    "    chunk_chars: int = 800,\n",
    "    img_ratio_when_hit: float = 0.15,\n",
    "    vlm_ratio_threshold: float = 0.10,     # ≥threshold：put images into chunk pass to VLM\n",
    "    max_images_per_chunk: int = 3,\n",
    "    keep_textless_image_chunks: bool = True,\n",
    ") -> List[Dict[str, Any]]:\n",
    "\n",
    "    doc = Document(path)\n",
    "\n",
    "    chunks: List[Dict[str, Any]] = []\n",
    "    buf: List[str] = []\n",
    "    buf_len = 0\n",
    "    buf_images: List[Image.Image] = []\n",
    "    section_id = 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal buf, buf_len, buf_images, section_id\n",
    "\n",
    "        text = \"\\n\".join([t for t in buf if t]).strip()\n",
    "        has_img = len(buf_images) > 0\n",
    "        image_area_ratio = img_ratio_when_hit if has_img else 0.0\n",
    "\n",
    "\n",
    "        images_for_chunk = []\n",
    "        if has_img and image_area_ratio >= vlm_ratio_threshold:\n",
    "            images_for_chunk = buf_images[:max_images_per_chunk]\n",
    "\n",
    "        keep = bool(text)\n",
    "        if (not keep) and keep_textless_image_chunks and has_img:\n",
    "            keep = True\n",
    "\n",
    "        if keep:\n",
    "            section_id += 1\n",
    "            chunks.append({\n",
    "                \"source_file\": path,\n",
    "                \"type\": \"docx\",\n",
    "                \"section\": section_id,\n",
    "                \"raw_text\": text,\n",
    "                \"image_area_ratio\": float(image_area_ratio),\n",
    "                \"route\": None,\n",
    "                \"images\": images_for_chunk,\n",
    "            })\n",
    "\n",
    "        # reset buffers\n",
    "        buf, buf_len, buf_images = [], 0, []\n",
    "\n",
    "    # traverse the document\n",
    "    for block in iter_block_items(doc):\n",
    "        if isinstance(block, Paragraph):\n",
    "            t = (block.text or \"\").strip()\n",
    "            if t:\n",
    "                buf.append(t)\n",
    "                buf_len += len(t)\n",
    "\n",
    "            # extract picture from paragraph\n",
    "            imgs = extract_images_from_paragraph(block)\n",
    "            if imgs:\n",
    "                buf_images.extend(imgs)\n",
    "\n",
    "        elif isinstance(block, Table):\n",
    "            table_text_parts = []\n",
    "            try:\n",
    "                for row in block.rows:\n",
    "                    row_text = []\n",
    "                    for cell in row.cells:\n",
    "                        cell_text = (cell.text or \"\").strip()\n",
    "                        if cell_text:\n",
    "                            row_text.append(cell_text)\n",
    "                    if row_text:\n",
    "                        table_text_parts.append(\" | \".join(row_text))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if table_text_parts:\n",
    "                tt = \"\\n\".join(table_text_parts).strip()\n",
    "                if tt:\n",
    "                    buf.append(tt)\n",
    "                    buf_len += len(tt)\n",
    "\n",
    "            # images in table\n",
    "            imgs = extract_images_from_table(block)\n",
    "            if imgs:\n",
    "                buf_images.extend(imgs)\n",
    "\n",
    "        if buf_len >= chunk_chars:\n",
    "            flush()\n",
    "\n",
    "    flush()\n",
    "    return chunks\n"
   ],
   "metadata": {
    "id": "gfSMevLhvrWQ"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "####2.4 Generation\n",
    "In this stage, I automatically detect the document type (PDF, PPTX, or DOCX), apply the corresponding extraction pipeline, and merge all extracted chunks into a unified list. All chunks follow a standardized schema, enabling cross-format embedding and retrieval."
   ],
   "metadata": {
    "id": "YvHJ2Z6sx1-L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# automatically scan document type and chunk\n",
    "all_chunks = []\n",
    "skipped = []\n",
    "\n",
    "for fname in uploaded.keys():\n",
    "    lower = fname.lower()\n",
    "    try:\n",
    "        if lower.endswith(\".pdf\"):\n",
    "            all_chunks += extract_pdf_chunks(fname)\n",
    "        elif lower.endswith(\".pptx\"):\n",
    "            all_chunks += extract_pptx_chunks(fname)\n",
    "        elif lower.endswith(\".docx\"):\n",
    "            all_chunks += extract_docx_chunks(fname)\n",
    "        else:\n",
    "            skipped.append(fname)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {fname}: {e}\")\n",
    "        skipped.append(fname)\n",
    "\n",
    "print(\"total chunks:\", len(all_chunks))\n",
    "if all_chunks:\n",
    "    print(\"Example metadata:\", {k: all_chunks[0][k] for k in all_chunks[0] if k != \"raw_text\"})\n",
    "    print(\"Text preview:\\n\", all_chunks[0][\"raw_text\"][:300])\n",
    "\n",
    "if skipped:\n",
    "    print(\"skipped/failed files:\", skipped)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05fY5DFcx3hs",
    "outputId": "86803776-542f-46b9-aad0-524086ee5e02"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total chunks: 98\n",
      "Example metadata: {'source_file': 'Lecture 5_IR_Websearch.pdf', 'type': 'pdf', 'page': 1, 'image_area_ratio': 0.0, 'route': None, 'page_image': None}\n",
      "Text preview:\n",
      " MH8351 Web Analytics\n",
      "An Brief Introduction of\n",
      "Information Retrieval and Web Search\n",
      "Li Xiaoli\n",
      "Nanyang Technological University\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#make decision: VLM or text\n",
    "def apply_router(chunks, vlm_ratio_threshold=0.10):\n",
    "    for c in chunks:\n",
    "        ratio = float(c.get(\"image_area_ratio\", 0.0) or 0.0)\n",
    "        c[\"route\"] = \"vlm\" if ratio >= vlm_ratio_threshold else \"direct\"\n",
    "    return chunks\n",
    "\n",
    "all_chunks = apply_router(all_chunks, vlm_ratio_threshold=0.10)\n",
    "\n",
    "from collections import Counter\n",
    "print(\"route counts:\", Counter([c[\"route\"] for c in all_chunks]))"
   ],
   "metadata": {
    "id": "akgf_JZ8iGes",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0a494b02-eab8-4d9b-c021-7a4b6c00cc2b"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "route counts: Counter({'direct': 71, 'vlm': 27})\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##3.VLM vision language model\n",
    "In this stage, I integrate a Vision-Language Model (VLM) to handle visual content that cannot be captured by pure text extraction. For each chunk that contains significant images, the corresponding page or slide is rendered into an image and passed to the VLM. The model generates a natural language caption describing the visual content (e.g., diagrams, charts, or illustrations). This caption is then appended to the original text chunk and used for downstream embedding and retrieval, enabling the system to understand and retrieve information from both textual and visual modalities."
   ],
   "metadata": {
    "id": "_6svCFpLoSl5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "vlm = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "vlm.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def caption_image(pil_img: Image.Image, max_new_tokens=50) -> str:\n",
    "    inputs = processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
    "    out = vlm.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return processor.decode(out[0], skip_special_tokens=True).strip()\n"
   ],
   "metadata": {
    "id": "5j9LFE-554B9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380,
     "referenced_widgets": [
      "fdb34a87725040d4b842ef61581909c2",
      "590f71a5498a46cf81e8dde8d58c70a8",
      "64ede32f63204e7c863e8cb14365fde5",
      "25c6f7340b6a4e839322dabe67b084ff",
      "b6f13f486a86496dbbb51bfd4b20ee98",
      "73974e7159bb40889df3e062f2a7dbc2",
      "a55c8e5b57a74611928cbd73bfe8636f",
      "9e54770e08ad4445965a7cce2b59b1fb",
      "f461d6ac94f54b5b954656e273a58ceb",
      "681755de1a5f43dfab805ef6afe05bb2",
      "3afb56c13590494aa0b9a37199305067",
      "0f61d67d833843f9896bcb3923d975fa",
      "19b72c08afd94d2690f77de8c76eb9ed",
      "a016056597da4be3a18ef2be5af24ca0",
      "77b1af7e729a4ccfb88be376b4038adf",
      "7a969c9beaa94517be5bba53f19cbe45",
      "2e17cfaee00b4b939c2a4c34e020c206",
      "d6fdb869a15c45d4ac3850615f6ebf1a",
      "07c81e3de93542b28c15f49214eec2d0",
      "b813fef3bc1b4d3bb0e6e63c7cb590e5",
      "3810ccec61a3410185d0e8ad5f2e15b6",
      "041e2b9ad8814efabae0e3bc5c6e0a67",
      "7f3ea27273f740afab0177b1391fa794",
      "0ab1d671f2b847d99f2bf53bd66b0a32",
      "100caa5b3ff34296abcfad62628bda3d",
      "b9a30ae53f094559849b820e89cb3208",
      "0c33e46170f14861bd66972aab949f6e",
      "452d91336238483fb471fb2c3dbda37f",
      "912fdcc2b8e94f65adb1ec18ed835807",
      "1970bdf733494dde90d4e76d1fff62d0",
      "dab6e118af5e41ddaa94cc28f49788bf",
      "d5753ca8fdd547398d79d34ec89997e0",
      "5d5efae23a884b71a9cbe0930207432b",
      "28bb721f35294964b12e8a7f099b9d07",
      "955d7e76c4994b348c6ee236507f871b",
      "eef5dc0004e546fe8e1a8dbbad79dfe8",
      "e25e9b88b2b241a49d71e51c62b7ed16",
      "6e5f994a67984f85bd7562974db2fb06",
      "c9edee2e7d2446f4bbca44f278e2d6d6",
      "e4ca8339c39b4a4a9b7b4b12202aa675",
      "af7be77dc892434285badc2ae629ebd8",
      "1c381535b84d401d801f5763600e32ec",
      "5491cf02eac1429ebe00625152b874bf",
      "64e1ce6a93724db5a86b53e26d718cda",
      "e363cd04c34e4328b1f67bf7586995ae",
      "bcc472384e264da0abfedc4f986ed265",
      "cbace52277144470a181c1d45c8a72ca",
      "f15880d63efd41eb8073b62f27cafb1a",
      "6042082d1622404b9588918505949321",
      "5b97341ebc5f43f09074068013908b1a",
      "2497f2216e284988bad8a6b620867102",
      "c902db8c220545a98a4895cef25da0be",
      "181ca1e807b849899259770bc1b97db2",
      "2037976ff9234787898024def632dae8",
      "a494740b22474ba786648ad682bbd476",
      "6d89e270602e450d9bcedae87def38d5",
      "facec53c92424e529eb4e0c147a2653e",
      "ef2ed5a863704f47b3e9303e456dfa0a",
      "7cc330ea387849da8641c7e8af1e55bc",
      "c5c50d52fe2244a58aa7ab7aaf1acace",
      "c6704ec6ca6e4f9fb856210a60128408",
      "c62fb48bce574e6aa7d97cffeab65ddb",
      "eb2431348e354872a77841a3da1d1a3a",
      "22f644e86c264a0289f3dbfc3cdabfa2",
      "414ae95a148f4495b703383cc719ef6d",
      "c215c2dc8fdd43dfbaf00afb1fdedd04",
      "b531414106f14513ab6ee83c6b8e84be",
      "ad28024a3ef54fdaa06012e0e1e4bbdb",
      "16fa8e4d9cb54039a24c53462c6a13f7",
      "755dd113949a488cb60bf8c17d1073c9",
      "19a380a29f004992b20ed27e2ccfc33f",
      "2abbe210931b42cfb4ae6dca09f619c0",
      "ac723512c57f4fa98182b8e1c1ed473c",
      "cfbf1d5d41b547298260f62263ae8b36",
      "f4a165b9b83443a99d9dd6678ebf88a0",
      "33b5e2baf386461fbdfbf1d98c3df4cd",
      "356f210fde4a49f7bc406c7a68aa18b9"
     ]
    },
    "outputId": "d48eb64c-36d0-408f-d20d-e1d1dea8eaf2"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "vlm = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "vlm.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def caption_image(pil_img: Image.Image, max_new_tokens=50) -> str:\n",
    "    inputs = processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
    "    out = vlm.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return processor.decode(out[0], skip_special_tokens=True).strip()\n"
   ],
   "metadata": {
    "id": "RqpJz71Og-LC"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def run_vlm_for_chunks(chunks, *, max_images_per_chunk=2):\n",
    "    for c in chunks:\n",
    "        if c.get(\"route\") != \"vlm\":\n",
    "            continue\n",
    "\n",
    "        doc_type = c.get(\"type\")\n",
    "        cap_texts = []\n",
    "\n",
    "        try:\n",
    "            if doc_type == \"pdf\":\n",
    "\n",
    "                img = c.get(\"page_image\")\n",
    "                if img is not None:\n",
    "                    cap_texts.append(caption_image(img))\n",
    "\n",
    "            elif doc_type in (\"pptx\", \"docx\"):\n",
    "\n",
    "                imgs = c.get(\"images\") or []\n",
    "                for img in imgs[:max_images_per_chunk]:\n",
    "                    cap_texts.append(caption_image(img))\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            c[\"image_caption\"] = None\n",
    "            c[\"vlm_error\"] = str(e)\n",
    "            continue\n",
    "\n",
    "        c[\"image_caption\"] = \"\\n\".join([t for t in cap_texts if t]).strip() if cap_texts else None\n",
    "\n",
    "    return chunks\n",
    "\n",
    "all_chunks = run_vlm_for_chunks(all_chunks, max_images_per_chunk=2)\n"
   ],
   "metadata": {
    "id": "9zVj6MGMyTAM"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "check"
   ],
   "metadata": {
    "id": "1rhHgKu4hyp8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "print(Counter([c[\"route\"] for c in all_chunks]))\n",
    "print(\"vlm captions:\", sum(1 for c in all_chunks if c.get(\"image_caption\")))\n"
   ],
   "metadata": {
    "id": "ylcHaM6Qhx96",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "667519de-b980-47c0-eb8f-c97389f7fd92"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'direct': 71, 'vlm': 27})\n",
      "vlm captions: 27\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for c in all_chunks:\n",
    "    if c.get(\"image_caption\"):\n",
    "        print(c[\"type\"], c.get(\"page\") or c.get(\"slide\"), c[\"image_caption\"][:200])\n",
    "        break"
   ],
   "metadata": {
    "id": "j1fHDnc0h0iP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "999592d2-4263-4a16-e490-b4da9e101e1d"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pdf 4 a diagram of a block diagram\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##4.Embedding"
   ],
   "metadata": {
    "id": "7Wq0BeAphrBa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_embedding_text(c):\n",
    "    loc = c.get(\"page\") or c.get(\"slide\") or c.get(\"section\") or \"\"\n",
    "    header = f\"[{c.get('type','doc')}:{loc}]\"\n",
    "\n",
    "    parts = [header]\n",
    "\n",
    "    raw = (c.get(\"raw_text\") or \"\").strip()\n",
    "    if raw:\n",
    "        parts.append(raw)\n",
    "\n",
    "    cap = (c.get(\"image_caption\") or \"\").strip()\n",
    "    if cap:\n",
    "        parts.append(\"[VLM]\\n\" + cap)\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "for c in all_chunks:\n",
    "    c[\"embedding_text\"] = build_embedding_text(c)\n",
    "\n",
    "print(all_chunks[0][\"embedding_text\"][:500])\n"
   ],
   "metadata": {
    "id": "pj9jG8G6hYF6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cec41dd1-a904-4b62-fa66-598af99c1afa"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[pdf:1]\n",
      "\n",
      "MH8351 Web Analytics\n",
      "An Brief Introduction of\n",
      "Information Retrieval and Web Search\n",
      "Li Xiaoli\n",
      "Nanyang Technological University\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##5.Sentence-Transformers"
   ],
   "metadata": {
    "id": "N2w-3Ny8jWXR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss, json\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 1.texts + id_map escape the empty\n",
    "texts_all = [(c.get(\"embedding_text\") or \"\").strip() for c in all_chunks]\n",
    "id_map = [i for i,t in enumerate(texts_all) if t]\n",
    "texts = [texts_all[i] for i in id_map]\n",
    "\n",
    "# 2.encode\n",
    "emb = embed_model.encode(\n",
    "    texts,\n",
    "    batch_size=64,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "emb = np.asarray(emb, dtype=\"float32\")\n",
    "\n",
    "print(\"emb shape:\", emb.shape)\n",
    "\n",
    "# 3.build faiss\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "print(\"index size:\", index.ntotal)\n",
    "\n",
    "# 4.save\n",
    "import json\n",
    "\n",
    "def make_json_safe_chunk(c: dict):\n",
    "#copy\n",
    "    d = dict(c)\n",
    "\n",
    "    # delete all of PIL.Image\n",
    "    for k in [\"images\", \"pil_images\", \"image\", \"page_image\", \"slide_images\"]:\n",
    "        if k in d:\n",
    "            d.pop(k, None)\n",
    "\n",
    "    for k, v in list(d.items()):\n",
    "        try:\n",
    "            json.dumps(v)\n",
    "        except TypeError:\n",
    "            d[k] = str(type(v))\n",
    "\n",
    "    return d\n",
    "\n",
    "safe_chunks = [make_json_safe_chunk(c) for c in all_chunks]\n",
    "\n",
    "json.dump(\n",
    "    {\"id_map\": id_map, \"chunks\": safe_chunks},\n",
    "    open(\"store.json\", \"w\"),\n",
    "    ensure_ascii=False\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435,
     "referenced_widgets": [
      "c2ba36abcc4a4f2b8ba353fb6f6f9ac3",
      "6358d196e2764e1c9287f3f1329c515f",
      "645cbbc925ed4921953fa97f9219de78",
      "c0e09532891540caafa43e23862dc0d7",
      "7f29e9bf65434d47bbd3a1c940832e49",
      "73f1cf60df2f48aa8e2a6958e7acd4d3",
      "7fe932e7591d42a8a92860272f0fcbe8",
      "a44c04588fa54115ad6e6c2e5a967986",
      "de58d6f4de8d4924b7b840ad76549ac8",
      "a4a1d190c89642cdb3d192699a43dd38",
      "7b349773b4164720a4b58f57471f5e10",
      "e396cfaa5a884619a8f83c468b530520",
      "0377efaff5f0428eb865607d667d897e",
      "ebbfb1b02fb0408dab543d03fbda1c3b",
      "cd50a2eec91642759f677d32d64edef1",
      "6658d7ff3b114c08b3cfc52185641443",
      "b16b1c73ea024c4ba3dcc5c7c0037162",
      "eb1b93352edd489bad0817fe74b15082",
      "923100a1b92c4dae88fc4497e6eb6a48",
      "9d59925dd9fb49d3afa699d987fcca0f",
      "5d6f6d6c630d465fae35156be1c788f4",
      "d10194fcbc094c96a36dd0c1620a26ce",
      "b864db2d183c496da3b8298c475ac8d6",
      "3e57a414e9ab48cdada7a54f0490ddfe",
      "c5bc87e203c948d194aac7c83d2e580c",
      "cdf96f261dfb485486e7e71a84c4239d",
      "9c92e6e4e5b1488385626eef72a97dfc",
      "69499937c250422b85c6584ca1e24978",
      "6ebb851eb2624a97a8ec01448fc7e91d",
      "26b14e987d3a45179d9cdfe5b2a5befb",
      "b6bc4680adcb46a994abe4239e81cc7e",
      "01310c8d99924620ac369282b221736c",
      "6d30f0b056a444ceb2bba11863e6a912",
      "2321021bda45466aa80d7a1cf3d648fc",
      "b2d27b5d5d1c4adabca9f2ee5cc836eb",
      "acd5756f30fd49bea0b6444de658f88e",
      "628cb5a097aa47fb92e0ec744a8a5515",
      "28b2e5aad5cd4ac287f33f2725904502",
      "448b371c43eb418593ee375d9d46671d",
      "665e7900f9b441a9be1b7d14b033fa8c",
      "0ff8614bdb4a4f688abf86cf3581c37e",
      "5099d80bb833412a9067e0749f2fb744",
      "0cca2de38b3b4764a456f0266dc4e1d3",
      "4463c0bca2c349b68907a52381682b8d",
      "98fc4286cbed421a95d4af692159dca8",
      "d1428a53c45942898f0d32009d9c7117",
      "c001ae8494d94256bad4423becfbfe0f",
      "f155409ce0b04bec9bcefb3301c12876",
      "20622536ca1941c2bb1c43b0ea19436b",
      "64502af537c94670ac6f9266f8dccff3",
      "84c0ca85673a4776a7c0385496f1a09f",
      "42df95d8244a4b0da92b838dc18a6acf",
      "8e8b1a3ddb434653afdcf9e512404961",
      "9006c712878d47d2aa839b8ff6ddceab",
      "82b4911480664c97a4b6ab03895b23c2",
      "c2dfe0283fc94d7a91dfb153f38200b3",
      "d8a4e88c485341a0bb671e67a61321fd",
      "aec8384b85c54eb2b03e4592eff174e7",
      "850af94b0dc64599be1fc9a36b7a2fd6",
      "1eaa9c5a7028499e94a0417b94b7674d",
      "d609d787557a4b7dac60680086eaf686",
      "42d3afb781a34858922e9e5677dcf37b",
      "58ec1f3591484c919c658f62a89b8b39",
      "efe4a2fe6b2545579000accc1786346b",
      "aed81668fd0244a4b1c15db40652da3d",
      "227a2df7eeaa4d99a1a5af131f6709a6",
      "a6881151755845c894217fb98bbdbe8a",
      "bbd0d16e465d4a0280b6019ed2f6cc14",
      "0db2f4376af54912b61b8007de9bc058",
      "f570cfbb2152489798db17da23698b0a",
      "a1e3f88d86ba486bb76c6602e55fa889",
      "826fc05b28a84c8cbf367f00a3f1f6b2",
      "5e829feec1714e9faaebba3909f47a33",
      "a9d93e98caf44763998e78860276bb79",
      "d1e29971aef0413588e5ecd51e1f4505",
      "94b4cf7850be4e9eb7d009dcba79e4ff",
      "4da6445c10634584aa251768a0d75677",
      "ad1978044a684b34b72b4c6117194ec0",
      "f57ee519f344474aa4e35d0aa112e0e4",
      "3e33fcc2823a4c92a9e91f9cc5f6b5fa",
      "7b201ab463b54c579e837bcc2fdbc772",
      "fab108aaff974707ab121e9d57981a83",
      "776f3fdc8db9430da28b9b77a4ad9f16",
      "d34559dc5bd44f779370d17fa38f6f58",
      "9ce9b6c1ff954c1f9903c30c92b76bea",
      "f85a9c0496344e2391cb85ecbb4124cc",
      "32af5ef1381440ef8423d422d72499f1",
      "7d90e0f713e64bbda315632524b8abed",
      "ed0add62a06b482cb8321ff6c33dd775",
      "605ce58ca3d44831ae558591e5f2e746",
      "d580a987fbd145909636b2f45729d76a",
      "fca082cef38a45608469fd26e8fa1db7",
      "c3315ac9b68f49d3ae72bc06fb1d7e0b",
      "5db62931bb2147c093180842a78354d6",
      "4cd66537d7374266b0f516e0ea64f476",
      "ad058b62c32f4a80a80b08a9e5112372",
      "0f566170bd03487cb42a95d61c3ba807",
      "6618853e137747a2af62662c30b0eb5e",
      "187620d7c4a840d884fbc3e93defe490",
      "00677e7a6ce44559ab5e0c185cfb5994",
      "8696d4ab1f4c43c1834eaff91ee54ded",
      "b079b0d64b5747169ba9399931562fad",
      "181e79ff8ea1496495b4d9b982a4167e",
      "ff39fad99b2444909ad740b08742fba3",
      "86482cd730d94990b984c83978078327",
      "bb5a746a756048e681b62885708850a4",
      "3c2e4cab198f41ddbfa9d886d8c44c09",
      "f4589c00891045379a45c059712b134a",
      "d7e9e211cca84d16889258664ee154a0",
      "e7524d737ff24262ad2c8155620d9c47",
      "9a16271d776a495a9fe44f59480bf261",
      "fa87575910544c8590ad1e3917a8528c",
      "685c3cd5db5f45589ac0ccb7abfe03dd",
      "dcb1f9f34b5e4904ab746c6da87c8b0d",
      "6198690a1a6349fea317fd8adf7f6e81",
      "ba6688e2b0614dc29458999e41995191",
      "a9ddcdb844414b8a8240062fc0087155",
      "469bed24edc94703a0294f7315fb285e",
      "aef9deb1583c48a6b8de9a27185ffd7d",
      "3d5e50ae97b0433fa4841b60d605fead",
      "918597a610d6440cbafa74a21976fc1c",
      "bd54e2f8a8ef4b58b11ba9797f986e65",
      "c74b59072f4d46c493a82baa421f5e15",
      "98ba6a1be7fe4e67a14dc3a9b8a78181",
      "04d5c07467374d19a9595f0fe9a7d6c2",
      "b9fef6aec797400d8d87758ca61fa3cb",
      "9c57bd401e514b4d9db771d54185462a",
      "bf5530b8d38b40159170bc700c1cc2d9",
      "f19abf370ee440efb18ba358fa836f61",
      "0460dd1f16654b25b60d7cf3d17e75b2",
      "31253649fde148138168702469a81ea8",
      "ac16160af6024c2c8b16505171982c07"
     ]
    },
    "id": "xayzvpyQ18lW",
    "outputId": "ed4da697-7f33-442b-a0bc-b90125cf9eef"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "emb shape: (98, 384)\n",
      "index size: 98\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "\n",
    "print(\"index size:\", index.ntotal)\n"
   ],
   "metadata": {
    "id": "J1XW8jSkjSFi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3744c769-c7c0-4ac3-8462-d0542312a606"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "index size: 98\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "####5.1 Create FAISS Index + a little test\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "myyvxVs3jhY_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def retrieve(query, topk=5):\n",
    "    q = embed_model.encode([query], normalize_embeddings=True)\n",
    "    q = np.array(q, dtype=\"float32\")\n",
    "    scores, ids = index.search(q, topk)\n",
    "\n",
    "    results = []\n",
    "    for s, idx in zip(scores[0], ids[0]):\n",
    "        c = all_chunks[int(idx)]\n",
    "        loc = c.get(\"page\") or c.get(\"slide\") or c.get(\"section\")\n",
    "        results.append({\n",
    "            \"idx\": int(idx),\n",
    "            \"score\": float(s),\n",
    "            \"file\": c[\"source_file\"],\n",
    "            \"type\": c[\"type\"],\n",
    "            \"loc\": loc,\n",
    "            \"preview\": (c.get(\"raw_text\",\"\")[:180] + \"...\") if c.get(\"raw_text\") else \"\"\n",
    "        })\n",
    "    return results\n"
   ],
   "metadata": {
    "id": "bWV5WuTWjaN0"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def build_context(results, all_chunks, max_chars=1200):\n",
    "    blocks = []\n",
    "    for r in results:\n",
    "        c = all_chunks[r[\"idx\"]]\n",
    "        loc = c.get(\"page\") or c.get(\"slide\") or c.get(\"section\")\n",
    "        src = f\"{c['source_file']} | {c['type']} | {loc}\"\n",
    "\n",
    "        body = (c.get(\"embedding_text\") or c.get(\"raw_text\") or \"\")\n",
    "        body = body[:max_chars]\n",
    "\n",
    "        blocks.append(f\"[SOURCE]\\n{src}\\n[CONTENT]\\n{body}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n"
   ],
   "metadata": {
    "id": "F_kjafmY3Zmm"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results = retrieve(\"What's the main idea of this resource？\", topk=5)\n",
    "\n",
    "for r in results:\n",
    "    print(r)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PqwKcOH3bV4",
    "outputId": "9e4cd26d-fbe8-4b43-b059-76dd7121dca8"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'idx': 68, 'score': 0.48919400572776794, 'file': 'Lecture 5_IR_Websearch.pdf', 'type': 'pdf', 'loc': 69, 'preview': 'Summary\\n• We only give a brief introduction to IR. There are a large number of \\nother topics (although Words to Vectors or Docs to Vectors are \\nmore advance)\\n– Statistical language...'}\n",
      "{'idx': 2, 'score': 0.438303679227829, 'file': 'Lecture 5_IR_Websearch.pdf', 'type': 'pdf', 'loc': 3, 'preview': '1. Information Retrieval (IR)\\n• Conceptually, IR is the study of finding needed information. \\ni.e., IR helps users find information that matches their \\ninformation needs. \\n– Users ...'}\n",
      "{'idx': 73, 'score': 0.43030574917793274, 'file': 'Lecture 6 Sentimental Analysis_Short.pdf', 'type': 'pdf', 'loc': 4, 'preview': 'Introduction – user generated content\\n• Word-of-mouth on the Web\\n– One can express personal experiences and opinions on almost anything, at \\nreview sites, forums, discussion groups...'}\n",
      "{'idx': 87, 'score': 0.41655275225639343, 'file': 'Lecture 6 Sentimental Analysis_Short.pdf', 'type': 'pdf', 'loc': 18, 'preview': 'Objective – structure the unstructured\\n• Objective: Given an opinionated document, \\n– Discover all quintuples (ej, ak, soijkl, hi, tl), \\n• i.e., mine the five corresponding pieces ...'}\n",
      "{'idx': 74, 'score': 0.4123733341693878, 'file': 'Lecture 6 Sentimental Analysis_Short.pdf', 'type': 'pdf', 'loc': 5, 'preview': 'Introduction – Applications\\n• Businesses and organizations: product and service benchmarking. Market \\nintelligence. \\n– Business spends a huge amount of money to find consumer senti...'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "####5.2 Construct context structure for LLM"
   ],
   "metadata": {
    "id": "iGke-VlC3hBK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "context = build_context(results, all_chunks)\n",
    "print(context[:1000])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTZ9INKI3ew1",
    "outputId": "572de94e-c4a6-4337-841b-def2bbe635f9"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[SOURCE]\n",
      "Lecture 5_IR_Websearch.pdf | pdf | 69\n",
      "[CONTENT]\n",
      "[pdf:69]\n",
      "\n",
      "Summary\n",
      "• We only give a brief introduction to IR. There are a large number of \n",
      "other topics (although Words to Vectors or Docs to Vectors are \n",
      "more advance)\n",
      "– Statistical language model\n",
      "– Latent semantic indexing (LSI and SVD).\n",
      "• Many other interesting topics are not covered\n",
      "– Web search\n",
      "• Ranking: combining contents and hyperlinks\n",
      "• Index compression (In order to speed up the search, tries should reside in memory. \n",
      "Index compression aims to represent the same information with fewer bytes)\n",
      "– Combining multiple rankings and meta search \n",
      "70\n",
      "\n",
      "---\n",
      "\n",
      "[SOURCE]\n",
      "Lecture 5_IR_Websearch.pdf | pdf | 3\n",
      "[CONTENT]\n",
      "[pdf:3]\n",
      "\n",
      "1. Information Retrieval (IR)\n",
      "• Conceptually, IR is the study of finding needed information. \n",
      "i.e., IR helps users find information that matches their \n",
      "information needs. \n",
      "– Users express their information needs as queries\n",
      "• Historically, IR is about document retrieval, emphasizing \n",
      "document as the basic unit.\n",
      "– Fi\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.3 LLM-RAG"
   ],
   "metadata": {
    "id": "NZmjnGjbqF4g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#LLM load (Qwen 0.5B)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gen_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gen_model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "gen_model.eval()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700,
     "referenced_widgets": [
      "6786de09e3484887a967fd7c471baa19",
      "76a09193b9ba448e896bee5540840812",
      "475f97dace9e43b8b9b9c2bb400aa75f",
      "65454a79f1b3478cb7f2624e7604a0b0",
      "a3dfaca832214d05969cc8fc791ebeee",
      "51accc09510c4ca88331114a5fc4ffc7",
      "3bf846a8704046e890e060881df809d0",
      "01c4c27b4f2646e1b9215712344d69cc",
      "b9cb44e0f8174f4f933070f4c7896835",
      "1f4e46b55e9f4dc6a839c6bb7a154b4f",
      "c323465a21004f18a32541dc91a1a28e",
      "a5133804480246919cee831c77ca579d",
      "df447ed9caa34eaf9ad5fc2bcceb7dcd",
      "7a65783c54b14be1becdfe5b7ee89aaa",
      "3b3fcd3c67fb478486570610bd1b962c",
      "a5765436636f48e397461a15e5bb8491",
      "431b3e817c5f49dfb6fb1bf5e4fc4edc",
      "37250137b30b450295937f9dbde166ff",
      "7c7d7696c9c84589a6345dc8d603780a",
      "a37bcced6a82442fa87b7a8b1a45f879",
      "54d1f8d7a1894a6ea5b6569e3c3351c6",
      "cff129ed383a4bfd8fedc9e8993c6316",
      "c0cc2eb1218244e08b91ed46ceb7d043",
      "1d98959ac9ee495db760af27083e242a",
      "192a0b7bcad84f54895f422b4306213e",
      "7651be15659648c3aad1b5c4f0952502",
      "077372626efe414c98d8645e1238ab86",
      "4a77227514214c099c165134eac69102",
      "b31ba1117d1849fbb2c8bf2ff556f3f5",
      "debc1c89d07a491094d9ed482b6528bb",
      "7c8adf433f1f4cb4b6717afa87bb63b3",
      "46553aa39118422a8863928800a5954e",
      "ad6005433b874ee89a402b5aca908b04",
      "8518a2330e6044029bbcd5e9f42e4ad8",
      "77ec8e013939459cbe355914c1a8bb7c",
      "f5170ab3f2324cfe84b1bdd62fe0b7c3",
      "e14a95516dfa40f9bf48ce2a5ae97475",
      "c4187b7fcf914e6ba3d20de060c25274",
      "6d95bf0d878e486f8cff5c28d6a6ae11",
      "1268e54931dd44d39ebea85f657d62c7",
      "95b129dc9291459998f332d051210fef",
      "71acbd2bc1024611b4edbf7718ea194b",
      "3025c88697b74f4da271a26fc1e7c836",
      "1a1f023bea78461599b9bad04bc5d3fa",
      "09556d905e3c458b86cfc2b2a90a67fb",
      "65b24131a1c04b71904040bb9f6c42af",
      "5f4f27c9a6304e3a8b3fdb33a7d590fd",
      "1e9567fe336f483e99fa5177ba0af555",
      "d9317c06206c4ada945076f285f70f26",
      "d71973edd60c4e659f4251cd9ed6e6e7",
      "449e8ef6d1a84e2faf1cecf6852b9e80",
      "770410436b3743faa9ae20bf6d289aa0",
      "dbc5e691686f49d6ad5b8bcb9a460395",
      "5625cd76890b486fa9d59a786c7daf21",
      "53bfcf7e92714fd89128519a997cdbec",
      "563e0628568744a99ea91da7ce323849",
      "2d2d2a1f91474746ad993045829f1898",
      "b2c5493835694234b433f4b9eb2011cf",
      "df53f517d22c421481bbae00b859d436",
      "b63f430668344e9ea632ccae712c3dd0",
      "a2fefa9773e347c19bd2f9c24f5ffb13",
      "f3495e7e4a6b48c3899c64ebe2eb9fb6",
      "1e8ffe002e184ef9bb28e1d591a8b8d1",
      "ca53d8e0f4fb4cca89b150cfd626a970",
      "f40a919b3c2b45c7bebbc1daf3ee19af",
      "34a187b3386f48d39fb86933ef859f25",
      "9df8e6b203c84de8b08e56c977c1722f",
      "4b4edf769cfa4b2e8789693182c2898f",
      "c58e90253b4345e7b46f6f748013ed3c",
      "1ad13815e6174252b52e020b241343ba",
      "9074921d09fd4a9cbaf980fd047447b6",
      "192186ec8a8d4e6398616b6fd0bf6430",
      "6b6d711f13574bafa2dd775bf781581e",
      "175e5239d07544cca2cfca8d4d4d1157",
      "94adcfb4308d4f5a89b42c90b777e9d1",
      "8681a74434d04caa811ed80cf115b7c9",
      "61de25b6a57f478eaec917401da2d352"
     ]
    },
    "id": "zKTwDnne4Vrf",
    "outputId": "93bfda20-f226-43e1-9ddc-099c60d7cf8f"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def llm_generate(prompt, max_new_tokens=300):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(gen_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[-1].strip()\n",
    "\n",
    "    return text\n"
   ],
   "metadata": {
    "id": "ABUf3nmw843i"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#retrieve (FAISS)\n",
    "import numpy as np\n",
    "\n",
    "def retrieve(query: str, topk=5):\n",
    "    q = embed_model.encode([query], normalize_embeddings=True)\n",
    "    q = np.array(q, dtype=\"float32\")\n",
    "    scores, ids = index.search(q, topk)\n",
    "\n",
    "    results = []\n",
    "    for s, idx in zip(scores[0], ids[0]):\n",
    "        idx = int(idx)\n",
    "        c = all_chunks[idx]\n",
    "        loc = c.get(\"page\") or c.get(\"slide\") or c.get(\"section\")\n",
    "        results.append({\n",
    "            \"idx\": idx,\n",
    "            \"score\": float(s),\n",
    "            \"file\": c.get(\"source_file\"),\n",
    "            \"type\": c.get(\"type\"),\n",
    "            \"loc\": loc,\n",
    "            \"preview\": (c.get(\"raw_text\",\"\")[:180] + \"...\") if c.get(\"raw_text\") else \"\"\n",
    "        })\n",
    "    return results"
   ],
   "metadata": {
    "id": "-bn6Ivdt4VuQ"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# build_context (use embedding_text with VLM)\n",
    "def build_context(results, all_chunks, max_chars_each=1200):\n",
    "    blocks = []\n",
    "    for r in results:\n",
    "        c = all_chunks[r[\"idx\"]]\n",
    "        loc = c.get(\"page\") or c.get(\"slide\") or c.get(\"section\")\n",
    "        src = f\"{c.get('source_file')} | {c.get('type')} | {loc}\"\n",
    "\n",
    "        body = (c.get(\"embedding_text\") or c.get(\"raw_text\") or \"\")\n",
    "        body = body.strip()[:max_chars_each]\n",
    "\n",
    "        blocks.append(f\"[SOURCE]\\n{src}\\n[CONTENT]\\n{body}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)"
   ],
   "metadata": {
    "id": "z1PsDQGc4VxJ"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# prompt template\n",
    "def make_prompt(question, context):\n",
    "    return f\"\"\"\n",
    "You are a careful and factual assistant.\n",
    "\n",
    "Use ONLY the context below to answer the question.\n",
    "If the context is insufficient, say: \"Insufficient information\" and explain what is missing.\n",
    "Do NOT repeat the context.\n",
    "Do NOT repeat the question.\n",
    "Do NOT repeat these instructions.\n",
    "\n",
    "Answer concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "vt3ukii14VzN"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# rag_answer\n",
    "def rag_answer(question: str, topk=5, max_new_tokens=300, max_chars_each=1200):\n",
    "    results = retrieve(question, topk=topk)\n",
    "    context = build_context(results, all_chunks, max_chars_each=max_chars_each)\n",
    "    prompt = make_prompt(question, context)\n",
    "    answer = llm_generate(prompt, max_new_tokens=max_new_tokens)\n",
    "    return answer, results\n",
    "\n",
    "\n",
    "# test\n",
    "ans, hits = rag_answer(\"What's the main idea of these two resources？\", topk=5, max_new_tokens=250)\n",
    "print(ans)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arUl5GyE4vjM",
    "outputId": "b9ad8ba2-aecd-4fe9-b016-1c3efc7bf1fc"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The main ideas of these two resources are:\n",
      "\n",
      "1. Lecture 5_IR_Websearch.pdf focuses on introducing the basics of Information Retrieval (IR), including concepts like proximity queries, full document queries, and information retrieval principles. It also discusses the importance of ranking and indexing for efficient search operations.\n",
      "\n",
      "2. Lecture 5_IR_Websearch.pdf includes three examples of search engines, discussing their indexing strategies, query processing methods, ranking algorithms, and the challenges involved in maintaining such systems. These examples highlight the complexity and technical intricacies involved in building and managing an effective web search engine. \n",
      "\n",
      "Both resources aim to provide foundational knowledge about how search engines work and how to improve their performance through advanced techniques such as LSI, SVD, and tries. However, there are significant differences between them, reflecting the diverse nature of search engine development and the varying approaches taken by different researchers and companies. For instance, Lecture 5_IR_Websearch.pdf focuses more on the theoretical aspects of IR while Lecture 5_IR_Websearch.pdf includes practical applications and case studies from different search engines. Understanding these distinctions can help students and practitioners make informed decisions about which resource to use when learning about search engine technology. [Insufficient information]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ans, hits = rag_answer(\"What does Lecture 6 say? Please summarize.\", topk=5)\n",
    "print(ans)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88hjgSN86zOA",
    "outputId": "370db644-17d8-4416-f14f-90a35f29dce0"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The lecture discusses sentiment analysis and provides examples from different sources like web search results, poetry, and Wikipedia. It highlights the importance of understanding how words are used together to infer meaning and sentiments about people's opinions and emotions towards things. Additionally, the lecture uses distributional hypothesis to analyze words and their usage patterns. Finally, it demonstrates sentiment analysis using a real-world dataset and visualizations like diagrams and screenshots. Overall, the lecture aims to provide insights into human communication and emotions through language. [INSUFFICIENT INFORMATION] The lecture does not contain any specific content related to the given context. [INSUFFICIENT INFORMATION] The lecture does not mention any specific source or topic related to the given context. [INSUFFICIENT INFORMATION] The lecture does not include any detailed analysis or summaries on the given topic. [INSUFFICIENT INFORMATION] There is no provided content or topic in the given context that can be directly attributed to the lecture's main focus. [INSUFFICIENT INFORMATION] Insufficient information. The lecture is focused on analyzing the use of words in different sources but does not provide a comprehensive summary or analysis of its main points. [INSUFFICIENT INFORMATION] The lecture does not cover the given topic comprehensively. [INSUFFICIENT INFORMATION] Insufficient information. The lecture does not have enough details or examples to fully summarize its content. [INSUFFICIENT INFORMATION] Insufficient information. The lecture lacks a clear and concise summary of its main points or findings. [INSUFFICIENT INFORMATION]\n"
     ]
    }
   ]
  }
 ]
}